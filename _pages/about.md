---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<!-- Remove or fix the following block -->
<!-- Optionally remove this block if it's not needed -->
<!--
{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}
-->

Hi there ğŸ‘‹ï¼ŒI am a second-year CS PhD student at the <a href="https://www.nd.edu/" target="_blank">University of Notre Dame</a>. I am fortunate to have <a href="https://engineering.nd.edu/faculty/xiangliang-zhang/" target="_blank">Professor Xiangliang Zhang</a> as my advisor and grateful for the guidance of <a href="https://www.yapengtian.com/" target="_blank">Professor Yapeng Tian</a>.  

Before joining Notre Dame, I earned my B.E. degree in Electrical Engineering from <a href="https://en.bjtu.edu.cn/" target="_blank">Beijing Jiaotong University</a> and an M.S. in Research from <a href="https://www.columbia.edu/" target="_blank">Columbia University</a>. I also interned at <a href="https://opencv.org/" target="_blank">Intelâ€™s OpenCV organization</a>, working with <a href="https://github.com/dmatveev" target="_blank">Dmitry Matveev</a>.  

My research interests include multimodal learning, focusing on leveraging multimodal data (e.g., language, vision, and audio) to interact with the physical world.


# ğŸ”¥ News
- *2024.11*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by WSDM Demo Track 2025.
- *2024.10*: Our new paper about Multimodal modal Machine Unlearning is now available on arxiv! Reading list <a href="https://arxiv.org/abs/2410.23330" target="_blank">here</a>.
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ One first-author paper accepted by EMNLP 2024.
- *2024.05*: I have passed qualify exam.
- *2024.05*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by ACL 2024.
- *2023.10*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by EMNLP 2023.
- *2023.06*: I started open source project woking at Intel OpenCV.org.

# ğŸ“ Publications 
- ``Arxiv 2024`` [CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP](https://arxiv.org/abs/2410.23330), **Tianyu Yang**, Lisen Dai, Zheyuan Liu, Xiangqi Wang, Meng Jiang, Yapeng Tian, Xiangliang Zhang.
- ``EMNLP 2024`` [SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answer](https://arxiv.org/abs/2411.04933), **Tianyu Yang**, Yiyang Nan, Lisen Dai, Zhenwen Liang, Tianya Peng and Xiangliang Zhang.
- ``WSDM 2025`` WildlifeLookup: A Chatbot Facilitating Wildlife Management with Accessible Data and Insights, Xiangqi Wang, **Tianyu Yang**,Jason Rohr,  Brett Scheffers, Nitesh Chawla and Xiangliang Zhang.
- ``ACL 2024`` [SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark](https://arxiv.org/abs/2402.05138), Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, **Tianyu Yang**, Jiajun Jiao, Renjie Pi, Jipeng Zhang, and Xiangliang Zhang.  
- ``EMNLP 2023`` [UniMath: A Foundational and Multimodal Mathematical Reasoner](https://aclanthology.org/2023.emnlp-main.440/), Zhenwen Liang, **Tianyu Yang**, Jipeng Zhang, and Xiangliang Zhang.  



<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->


# ğŸ“– Educations
- *2023.09 â€“ Present*, **University of Notre Dame, South Bend, IN**  
  PhD in Computer Science
- *2019.09 â€“ 2021.05*, **Columbia University, New York, NY**  
  Research MS in Electrical Engineering
- *2017.09 â€“ 2021.05*, **Beijing Jiaotong University, Beijing, China**  
  BS in Automation
  
<!-- # ğŸ– Honors and Awards
- *2023*, Summer Research Scholarship at University of Notre Dame  
- *2021*, MS Elective Research Scholarship at Columbia University   
- *2020*, **First-Class**: National Mathematical Contest in Modeling  
- *2020*, **First-Class**: Academic Scholarships of BJTU 
- *2018*, **National**: China National Scholarship  -->
  
# ğŸ’¬ Personal Service
- *2025*, Reviewer at NAACL  
- *2024*, Reviewer at EMNLP  
- *2024*, Reviewer at ACL  
- *2024*, Reviewer at ICDM
- *2024*, Undergraduate Mentor at the Department of Computer Science, Notre Dame
- *2023*, Teaching Assistant for Advanced Algorithms at the Department of Computer Science, Columbia University
- *2023*, Teaching Assistant for Reinforcement Learning at the Department of Computer Science, Columbia University 

# ğŸ’» WORK EXPERIENCE
- *2023.05 â€“ 2023.08*, **Intel & OpenCV.org (Google Summer of Code)**  
  Mentor: Dmitry Matveev  
 
